{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import OrderedDict \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown, gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bible-kjv.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples = gutenberg.sents(gutenberg.fileids()[3])\n",
    "pattern = re.compile(\"[A-Za-z]+\")\n",
    "stop_w =  set(stopwords.words('english'))\n",
    "corpus = []\n",
    "for sent in samples:\n",
    "    sent = [w.lower() for w in sent]\n",
    "    sent = [w for w in sent if w not in stop_w]\n",
    "    sent = [w.replace('\\n', ' ') for w in sent]\n",
    "    sent = [w for w in sent if pattern.fullmatch(w)]\n",
    "    if len(sent) > 5:\n",
    "        corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25481"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples) # 문장의 수가 3만개임\n",
    "len(corpus) # 실제 문장의 수는 corpus에 들어가있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fre_dist = FreqDist()\n",
    "for sent in corpus:\n",
    "    fre_dist.update(sent)\n",
    "fre_dist = {k : v for k, v in fre_dist.items() if v > 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gihon', 6),\n",
       " ('heel', 6),\n",
       " ('firstlings', 6),\n",
       " ('sevenfold', 6),\n",
       " ('jared', 6),\n",
       " ('methuselah', 6),\n",
       " ('seventeenth', 6),\n",
       " ('restrained', 6),\n",
       " ('husbandman', 6),\n",
       " ('salah', 6),\n",
       " ('commended', 6),\n",
       " ('plagued', 6),\n",
       " ('dwelled', 6),\n",
       " ('arioch', 6),\n",
       " ('eshcol', 6),\n",
       " ('thread', 6),\n",
       " ('childless', 6),\n",
       " ('pleaseth', 6),\n",
       " ('shur', 6),\n",
       " ('hastened', 6),\n",
       " ('blindness', 6),\n",
       " ('withheld', 6),\n",
       " ('bondwoman', 6),\n",
       " ('knife', 6),\n",
       " ('kirjatharba', 6),\n",
       " ('machpelah', 6),\n",
       " ('castles', 6),\n",
       " ('twins', 6),\n",
       " ('bashemath', 6),\n",
       " ('elon', 6),\n",
       " ('savoury', 6),\n",
       " ('smooth', 6),\n",
       " ('subtilty', 6),\n",
       " ('mandrakes', 6),\n",
       " ('leaped', 6),\n",
       " ('quite', 6),\n",
       " ('stole', 6),\n",
       " ('parcel', 6),\n",
       " ('shammah', 6),\n",
       " ('dishon', 6),\n",
       " ('wandering', 6),\n",
       " ('rid', 6),\n",
       " ('overseer', 6),\n",
       " ('uppermost', 6),\n",
       " ('forgat', 6),\n",
       " ('knee', 6),\n",
       " ('storehouses', 6),\n",
       " ('bondman', 6),\n",
       " ('changes', 6),\n",
       " ('laden', 6),\n",
       " ('revived', 6),\n",
       " ('jamin', 6),\n",
       " ('tola', 6),\n",
       " ('stooped', 6),\n",
       " ('wolf', 6),\n",
       " ('physicians', 6),\n",
       " ('babe', 6),\n",
       " ('spied', 6),\n",
       " ('leprous', 6),\n",
       " ('demanded', 6),\n",
       " ('izhar', 6),\n",
       " ('sorcerers', 6),\n",
       " ('bedchamber', 6),\n",
       " ('lice', 6),\n",
       " ('thunderings', 6),\n",
       " ('raw', 6),\n",
       " ('sodden', 6),\n",
       " ('dances', 6),\n",
       " ('mete', 6),\n",
       " ('meribah', 6),\n",
       " ('quit', 6),\n",
       " ('restitution', 6),\n",
       " ('sacrificeth', 6),\n",
       " ('devouring', 6),\n",
       " ('dyed', 6),\n",
       " ('tongs', 6),\n",
       " ('tenons', 6),\n",
       " ('grate', 6),\n",
       " ('girdles', 6),\n",
       " ('bonnets', 6),\n",
       " ('thumb', 6),\n",
       " ('toe', 6),\n",
       " ('waved', 6),\n",
       " ('remainder', 6),\n",
       " ('self', 6),\n",
       " ('cloudy', 6),\n",
       " ('snuffers', 6),\n",
       " ('plates', 6),\n",
       " ('feathers', 6),\n",
       " ('pan', 6),\n",
       " ('flanks', 6),\n",
       " ('bewail', 6),\n",
       " ('reddish', 6),\n",
       " ('woollen', 6),\n",
       " ('mankind', 6),\n",
       " ('talebearer', 6),\n",
       " ('mar', 6),\n",
       " ('weights', 6),\n",
       " ('manners', 6),\n",
       " ('chase', 6),\n",
       " ('polls', 6),\n",
       " ('expressed', 6),\n",
       " ('hebronites', 6),\n",
       " ('abihail', 6),\n",
       " ('serveth', 6),\n",
       " ('lain', 6),\n",
       " ('rereward', 6),\n",
       " ('suffice', 6),\n",
       " ('speeches', 6),\n",
       " ('talmai', 6),\n",
       " ('zoan', 6),\n",
       " ('doubtless', 6),\n",
       " ('sticks', 6),\n",
       " ('blossom', 6),\n",
       " ('dying', 6),\n",
       " ('discouraged', 6),\n",
       " ('ar', 6),\n",
       " ('jeshimon', 6),\n",
       " ('perverseness', 6),\n",
       " ('unicorn', 6),\n",
       " ('valiantly', 6),\n",
       " ('kenite', 6),\n",
       " ('chittim', 6),\n",
       " ('baalpeor', 6),\n",
       " ('zarhites', 6),\n",
       " ('levy', 6),\n",
       " ('inherited', 6),\n",
       " ('meddle', 6),\n",
       " ('specially', 6),\n",
       " ('repay', 6),\n",
       " ('careth', 6),\n",
       " ('lusteth', 6),\n",
       " ('withdrawn', 6),\n",
       " ('sendest', 6),\n",
       " ('beheaded', 6),\n",
       " ('chastened', 6),\n",
       " ('chance', 6),\n",
       " ('plow', 6),\n",
       " ('edomite', 6),\n",
       " ('settest', 6),\n",
       " ('divorcement', 6),\n",
       " ('howling', 6),\n",
       " ('spend', 6),\n",
       " ('teareth', 6),\n",
       " ('firm', 6),\n",
       " ('lodging', 6),\n",
       " ('achan', 6),\n",
       " ('zabdi', 6),\n",
       " ('confession', 6),\n",
       " ('tappuah', 6),\n",
       " ('taanach', 6),\n",
       " ('distributed', 6),\n",
       " ('bay', 6),\n",
       " ('othniel', 6),\n",
       " ('shema', 6),\n",
       " ('gedor', 6),\n",
       " ('engedi', 6),\n",
       " ('bethshean', 6),\n",
       " ('dividing', 6),\n",
       " ('kishon', 6),\n",
       " ('key', 6),\n",
       " ('jael', 6),\n",
       " ('dropped', 6),\n",
       " ('caves', 6),\n",
       " ('retained', 6),\n",
       " ('zeeb', 6),\n",
       " ('secure', 6),\n",
       " ('ebed', 6),\n",
       " ('expounded', 6),\n",
       " ('tails', 6),\n",
       " ('delilah', 6),\n",
       " ('ropes', 6),\n",
       " ('teraphim', 6),\n",
       " ('wayfaring', 6),\n",
       " ('beset', 6),\n",
       " ('knit', 6),\n",
       " ('gleaned', 6),\n",
       " ('danced', 6),\n",
       " ('elimelech', 6),\n",
       " ('moabitess', 6),\n",
       " ('salmon', 6),\n",
       " ('caldron', 6),\n",
       " ('watching', 6),\n",
       " ('deadly', 6),\n",
       " ('lucre', 6),\n",
       " ('battles', 6),\n",
       " ('prophesying', 6),\n",
       " ('renew', 6),\n",
       " ('ceasing', 6),\n",
       " ('enlightened', 6),\n",
       " ('haply', 6),\n",
       " ('saveth', 6),\n",
       " ('goliath', 6),\n",
       " ('bolster', 6),\n",
       " ('nob', 6),\n",
       " ('debt', 6),\n",
       " ('greet', 6),\n",
       " ('pisseth', 6),\n",
       " ('sleeping', 6),\n",
       " ('compelled', 6),\n",
       " ('leaned', 6),\n",
       " ('swifter', 6),\n",
       " ('delights', 6),\n",
       " ('communication', 6),\n",
       " ('fort', 6),\n",
       " ('telleth', 6),\n",
       " ('chasten', 6),\n",
       " ('porter', 6),\n",
       " ('thinketh', 6),\n",
       " ('hatest', 6),\n",
       " ('boat', 6),\n",
       " ('ira', 6),\n",
       " ('gibeonites', 6),\n",
       " ('prevented', 6),\n",
       " ('hinds', 6),\n",
       " ('fade', 6),\n",
       " ('eliphelet', 6),\n",
       " ('shunammite', 6),\n",
       " ('pipes', 6),\n",
       " ('jether', 6),\n",
       " ('ethan', 6),\n",
       " ('costly', 6),\n",
       " ('furnished', 6),\n",
       " ('navy', 6),\n",
       " ('chastised', 6),\n",
       " ('sleepeth', 6),\n",
       " ('escapeth', 6),\n",
       " ('humbleth', 6),\n",
       " ('persuade', 6),\n",
       " ('joints', 6),\n",
       " ('swim', 6),\n",
       " ('lepers', 6),\n",
       " ('revolted', 6),\n",
       " ('worshippers', 6),\n",
       " ('chest', 6),\n",
       " ('judging', 6),\n",
       " ('seers', 6),\n",
       " ('samaritans', 6),\n",
       " ('trustest', 6),\n",
       " ('chamberlain', 6),\n",
       " ('forts', 6),\n",
       " ('jerahmeel', 6),\n",
       " ('tekoa', 6),\n",
       " ('azrikam', 6),\n",
       " ('hedges', 6),\n",
       " ('mahli', 6),\n",
       " ('malluch', 6),\n",
       " ('jediael', 6),\n",
       " ('malchijah', 6),\n",
       " ('immer', 6),\n",
       " ('jahaziel', 6),\n",
       " ('expert', 6),\n",
       " ('psalm', 6),\n",
       " ('weigh', 6),\n",
       " ('delaiah', 6),\n",
       " ('teacher', 6),\n",
       " ('jehohanan', 6),\n",
       " ('cups', 6),\n",
       " ('drams', 6),\n",
       " ('overseers', 6),\n",
       " ('arabians', 6),\n",
       " ('withstood', 6),\n",
       " ('bigvai', 6),\n",
       " ('pahathmoab', 6),\n",
       " ('bebai', 6),\n",
       " ('damage', 6),\n",
       " ('needful', 6),\n",
       " ('ariel', 6),\n",
       " ('meremoth', 6),\n",
       " ('blotted', 6),\n",
       " ('reading', 6),\n",
       " ('myrtle', 6),\n",
       " ('worshippeth', 6),\n",
       " ('ware', 6),\n",
       " ('aforetime', 6),\n",
       " ('contended', 6),\n",
       " ('media', 6),\n",
       " ('holdest', 6),\n",
       " ('durst', 6),\n",
       " ('pleasing', 6),\n",
       " ('temanite', 6),\n",
       " ('blackness', 6),\n",
       " ('mortal', 6),\n",
       " ('chastening', 6),\n",
       " ('forsaketh', 6),\n",
       " ('hireling', 6),\n",
       " ('wert', 6),\n",
       " ('shaketh', 6),\n",
       " ('rewardeth', 6),\n",
       " ('wet', 6),\n",
       " ('groan', 6),\n",
       " ('stretcheth', 6),\n",
       " ('clap', 6),\n",
       " ('cutteth', 6),\n",
       " ('jaws', 6),\n",
       " ('hiding', 6),\n",
       " ('wish', 6),\n",
       " ('profited', 6),\n",
       " ('oftentimes', 6),\n",
       " ('profiteth', 6),\n",
       " ('crowned', 6),\n",
       " ('horrible', 6),\n",
       " ('bountifully', 6),\n",
       " ('greedy', 6),\n",
       " ('correct', 6),\n",
       " ('prevent', 6),\n",
       " ('hoofs', 6),\n",
       " ('assemblies', 6),\n",
       " ('suffering', 6),\n",
       " ('likened', 6),\n",
       " ('memory', 6),\n",
       " ('laughter', 6),\n",
       " ('lowly', 6),\n",
       " ('attain', 6),\n",
       " ('decked', 6),\n",
       " ('goodman', 6),\n",
       " ('revealeth', 6),\n",
       " ('liberal', 6),\n",
       " ('feedeth', 6),\n",
       " ('furious', 6),\n",
       " ('thorn', 6),\n",
       " ('exercised', 6),\n",
       " ('happeneth', 6),\n",
       " ('bereave', 6),\n",
       " ('offences', 6),\n",
       " ('oaks', 6),\n",
       " ('soothsayers', 6),\n",
       " ('isle', 6),\n",
       " ('hypocrisy', 6),\n",
       " ('liken', 6),\n",
       " ('deserts', 6),\n",
       " ('mothers', 6),\n",
       " ('sower', 6),\n",
       " ('detestable', 6),\n",
       " ('ebedmelech', 6),\n",
       " ('aholibah', 6),\n",
       " ('doted', 6),\n",
       " ('fairs', 6),\n",
       " ('market', 6),\n",
       " ('blasphemies', 6),\n",
       " ('prospect', 6),\n",
       " ('informed', 6),\n",
       " ('cankerworm', 6),\n",
       " ('josedech', 6),\n",
       " ('agree', 6),\n",
       " ('mote', 6),\n",
       " ('tombs', 6),\n",
       " ('publican', 6),\n",
       " ('choked', 6),\n",
       " ('unfruitful', 6),\n",
       " ('herodias', 6),\n",
       " ('philippi', 6),\n",
       " ('fellowservant', 6),\n",
       " ('penny', 6),\n",
       " ('hosanna', 6),\n",
       " ('unprofitable', 6),\n",
       " ('barabbas', 6),\n",
       " ('forgiveness', 6),\n",
       " ('lasciviousness', 6),\n",
       " ('creation', 6),\n",
       " ('alexander', 6),\n",
       " ('beckoned', 6),\n",
       " ('highly', 6),\n",
       " ('dear', 6),\n",
       " ('honest', 6),\n",
       " ('examined', 6),\n",
       " ('commend', 6),\n",
       " ('cephas', 6),\n",
       " ('nathanael', 6),\n",
       " ('romans', 6),\n",
       " ('expedient', 6),\n",
       " ('unlearned', 6),\n",
       " ('iconium', 6),\n",
       " ('lystra', 6),\n",
       " ('galatia', 6),\n",
       " ('thessalonica', 6),\n",
       " ('corinth', 6),\n",
       " ('corruptible', 6),\n",
       " ('disobedience', 6),\n",
       " ('election', 6),\n",
       " ('weakness', 6),\n",
       " ('malice', 6),\n",
       " ('effectual', 6),\n",
       " ('appearing', 6),\n",
       " ('yielding', 7),\n",
       " ('subdue', 7),\n",
       " ('havilah', 7),\n",
       " ('enos', 7),\n",
       " ('cainan', 7),\n",
       " ('mahalaleel', 7),\n",
       " ('renown', 7),\n",
       " ('rooms', 7),\n",
       " ('pulled', 7),\n",
       " ('dread', 7),\n",
       " ('javan', 7),\n",
       " ('cush', 7),\n",
       " ('dedan', 7),\n",
       " ('shinar', 7),\n",
       " ('peleg', 7),\n",
       " ('brick', 7),\n",
       " ('nativity', 7),\n",
       " ('kenites', 7),\n",
       " ('hearth', 7),\n",
       " ('dressed', 7),\n",
       " ('ewe', 7),\n",
       " ('yonder', 7),\n",
       " ('buryingplace', 7),\n",
       " ('mesopotamia', 7),\n",
       " ('provender', 7),\n",
       " ('destitute', 7),\n",
       " ('pottage', 7),\n",
       " ('lightly', 7),\n",
       " ('hundredfold', 7),\n",
       " ('dim', 7),\n",
       " ('feel', 7),\n",
       " ('awaked', 7),\n",
       " ('luz', 7),\n",
       " ('zilpah', 7),\n",
       " ('spotted', 7),\n",
       " ('ringstraked', 7),\n",
       " ('jabbok', 7),\n",
       " ('softly', 7),\n",
       " ('stink', 7),\n",
       " ('zibeon', 7),\n",
       " ('got', 7),\n",
       " ('lotan', 7),\n",
       " ('ezer', 7),\n",
       " ('achbor', 7),\n",
       " ('obeisance', 7),\n",
       " ('seekest', 7),\n",
       " ('clusters', 7),\n",
       " ('ripe', 7),\n",
       " ('interpret', 7),\n",
       " ('hastily', 7),\n",
       " ('households', 7),\n",
       " ('welfare', 7),\n",
       " ('refrained', 7),\n",
       " ('jachin', 7),\n",
       " ('shaul', 7),\n",
       " ('couch', 7),\n",
       " ('lawgiver', 7),\n",
       " ('rider', 7),\n",
       " ('bough', 7),\n",
       " ('midwives', 7),\n",
       " ('daubed', 7),\n",
       " ('heretofore', 7),\n",
       " ('thunders', 7),\n",
       " ('destroyer', 7),\n",
       " ('lent', 7),\n",
       " ('hardly', 7),\n",
       " ('serving', 7),\n",
       " ('murmurings', 7),\n",
       " ('sittest', 7),\n",
       " ('causes', 7),\n",
       " ('fifties', 7),\n",
       " ('peculiar', 7),\n",
       " ('selleth', 7),\n",
       " ('bar', 7),\n",
       " ('shovels', 7),\n",
       " ('network', 7),\n",
       " ('jasper', 7),\n",
       " ('urim', 7),\n",
       " ('hem', 7),\n",
       " ('workmanship', 7),\n",
       " ('fashioned', 7),\n",
       " ('dancing', 7),\n",
       " ('shone', 7),\n",
       " ('weaver', 7),\n",
       " ('occupied', 7),\n",
       " ('inclosed', 7),\n",
       " ('turtledoves', 7),\n",
       " ('imputed', 7),\n",
       " ('toes', 7),\n",
       " ('mishael', 7),\n",
       " ('befallen', 7),\n",
       " ('creep', 7),\n",
       " ('scab', 7),\n",
       " ('shaven', 7),\n",
       " ('fret', 7),\n",
       " ('pertaineth', 7),\n",
       " ('plaister', 7),\n",
       " ('rideth', 7),\n",
       " ('customs', 7),\n",
       " ('rob', 7),\n",
       " ('playing', 7),\n",
       " ('beeves', 7),\n",
       " ('maimed', 7),\n",
       " ('crushed', 7),\n",
       " ('cow', 7),\n",
       " ('relieve', 7),\n",
       " ('stock', 7),\n",
       " ('flight', 7),\n",
       " ('pursueth', 7),\n",
       " ('odours', 7),\n",
       " ('value', 7),\n",
       " ('ability', 7),\n",
       " ('devoted', 7),\n",
       " ('gamaliel', 7),\n",
       " ('camps', 7),\n",
       " ('razor', 7),\n",
       " ('ethiopian', 7),\n",
       " ('zaccur', 7),\n",
       " ('grasshoppers', 7),\n",
       " ('reproacheth', 7),\n",
       " ('strip', 7),\n",
       " ('zippor', 7),\n",
       " ('abiding', 7),\n",
       " ('agag', 7),\n",
       " ('puttest', 7),\n",
       " ('javelin', 7),\n",
       " ('revenger', 7),\n",
       " ('figure', 7),\n",
       " ('temptations', 7),\n",
       " ('walkest', 7),\n",
       " ('soles', 7),\n",
       " ('exact', 7),\n",
       " ('wanteth', 7),\n",
       " ('useth', 7),\n",
       " ('diviners', 7),\n",
       " ('rough', 7),\n",
       " ('eggs', 7),\n",
       " ('sorts', 7),\n",
       " ('tokens', 7),\n",
       " ('bill', 7),\n",
       " ('noonday', 7),\n",
       " ('assurance', 7),\n",
       " ('drunkenness', 7),\n",
       " ('sky', 7),\n",
       " ('liars', 7),\n",
       " ('bethaven', 7),\n",
       " ('ambush', 7),\n",
       " ('issued', 7),\n",
       " ('slaying', 7),\n",
       " ('ambassadors', 7),\n",
       " ('jarmuth', 7),\n",
       " ('azekah', 7),\n",
       " ('dor', 7),\n",
       " ('eshtaol', 7),\n",
       " ('maon', 7),\n",
       " ('abiezer', 7),\n",
       " ('mentioned', 7),\n",
       " ('aijalon', 7),\n",
       " ('certainty', 7),\n",
       " ('sincerity', 7),\n",
       " ('forced', 7),\n",
       " ('spoilers', 7),\n",
       " ('message', 7),\n",
       " ('hammer', 7),\n",
       " ('pen', 7),\n",
       " ('bitterly', 7),\n",
       " ('oreb', 7),\n",
       " ('misery', 7),\n",
       " ('grind', 7),\n",
       " ('laish', 7),\n",
       " ('kinsmen', 7),\n",
       " ('raiseth', 7),\n",
       " ('dunghill', 7),\n",
       " ('struck', 7),\n",
       " ('chiefest', 7),\n",
       " ('meaneth', 7),\n",
       " ('palms', 7),\n",
       " ('lighten', 7),\n",
       " ('asking', 7),\n",
       " ('axes', 7),\n",
       " ('tasted', 7),\n",
       " ('ahinoam', 7),\n",
       " ('scrip', 7),\n",
       " ('sunk', 7),\n",
       " ('sheath', 7),\n",
       " ('something', 7),\n",
       " ('roe', 7),\n",
       " ('masons', 7),\n",
       " ('garrisons', 7),\n",
       " ('pelethites', 7),\n",
       " ('ittai', 7),\n",
       " ('azmaveth', 7),\n",
       " ('skill', 7),\n",
       " ('settled', 7),\n",
       " ('contain', 7),\n",
       " ('prayeth', 7),\n",
       " ('arabia', 7),\n",
       " ('sycomore', 7),\n",
       " ('shishak', 7),\n",
       " ('provocation', 7),\n",
       " ('stop', 7),\n",
       " ('shaphat', 7),\n",
       " ('hasten', 7),\n",
       " ('careful', 7),\n",
       " ('receiving', 7),\n",
       " ('dash', 7),\n",
       " ('wanting', 7),\n",
       " ('afore', 7),\n",
       " ('doers', 7),\n",
       " ('michaiah', 7),\n",
       " ('pedaiah', 7),\n",
       " ('elnathan', 7),\n",
       " ('craftsmen', 7),\n",
       " ('jeconiah', 7),\n",
       " ('jahath', 7),\n",
       " ('skilful', 7),\n",
       " ('meraioth', 7),\n",
       " ('opening', 7),\n",
       " ('helpers', 7),\n",
       " ('sounding', 7),\n",
       " ('degree', 7),\n",
       " ('shebaniah', 7),\n",
       " ('cornet', 7),\n",
       " ('harim', 7),\n",
       " ('kneeled', 7),\n",
       " ('realm', 7),\n",
       " ('perfected', 7),\n",
       " ('nehemiah', 7),\n",
       " ('rehum', 7),\n",
       " ('noble', 7),\n",
       " ('binnui', 7),\n",
       " ('manifold', 7),\n",
       " ('gallows', 7),\n",
       " ('feasting', 7),\n",
       " ('retain', 7),\n",
       " ('solitary', 7),\n",
       " ('sighing', 7),\n",
       " ('troops', 7),\n",
       " ('shutteth', 7),\n",
       " ('travaileth', 7),\n",
       " ('straitened', 7),\n",
       " ('reveal', 7),\n",
       " ('wrongfully', 7),\n",
       " ('innumerable', 7),\n",
       " ('oppressors', 7),\n",
       " ('furrows', 7),\n",
       " ('preserveth', 7),\n",
       " ('hardness', 7),\n",
       " ('throat', 7),\n",
       " ('swelling', 7),\n",
       " ('sunder', 7),\n",
       " ('overwhelmed', 7),\n",
       " ('melteth', 7),\n",
       " ('robbery', 7),\n",
       " ('forgave', 7),\n",
       " ('endured', 7),\n",
       " ('revive', 7),\n",
       " ('shortened', 7),\n",
       " ('tossed', 7),\n",
       " ('undefiled', 7),\n",
       " ('quickened', 7),\n",
       " ('housetops', 7),\n",
       " ('lofty', 7),\n",
       " ('skirts', 7),\n",
       " ('stony', 7),\n",
       " ('returneth', 7),\n",
       " ('outcasts', 7),\n",
       " ('ornament', 7),\n",
       " ('ravished', 7),\n",
       " ('humility', 7),\n",
       " ('housetop', 7),\n",
       " ('bite', 7),\n",
       " ('veil', 7),\n",
       " ('fadeth', 7),\n",
       " ('pomp', 7),\n",
       " ('grievously', 7),\n",
       " ('islands', 7),\n",
       " ('fishers', 7),\n",
       " ('noph', 7),\n",
       " ('treacherous', 7),\n",
       " ('sigh', 7),\n",
       " ('eunuch', 7),\n",
       " ('wastes', 7),\n",
       " ('warning', 7),\n",
       " ('principalities', 7),\n",
       " ('perfectly', 7),\n",
       " ('prophesieth', 7),\n",
       " ('evidence', 7),\n",
       " ('chaldea', 7),\n",
       " ('chebar', 7),\n",
       " ('wolves', 7),\n",
       " ('alienated', 7),\n",
       " ('languages', 7),\n",
       " ('belshazzar', 7),\n",
       " ('amos', 7),\n",
       " ('resist', 7),\n",
       " ('example', 7),\n",
       " ('danger', 7),\n",
       " ('reconciled', 7),\n",
       " ('gnashing', 7),\n",
       " ('tares', 7),\n",
       " ('tetrarch', 7),\n",
       " ('fragments', 7),\n",
       " ('agreed', 7),\n",
       " ('supposed', 7),\n",
       " ('wedding', 7),\n",
       " ('privately', 7),\n",
       " ('cock', 7),\n",
       " ('crow', 7),\n",
       " ('hall', 7),\n",
       " ('sprang', 7),\n",
       " ('virtue', 7),\n",
       " ('gladly', 7),\n",
       " ('ministration', 7),\n",
       " ('supposing', 7),\n",
       " ('selves', 7),\n",
       " ('stephen', 7),\n",
       " ('cilicia', 7),\n",
       " ('sirs', 7),\n",
       " ('fervent', 7),\n",
       " ('edifying', 7),\n",
       " ('timothy', 7),\n",
       " ('bottomless', 7),\n",
       " ('moveth', 8),\n",
       " ('enmity', 8),\n",
       " ('bruise', 8),\n",
       " ('adah', 8),\n",
       " ('seth', 8),\n",
       " ('tubal', 8),\n",
       " ('meshech', 8),\n",
       " ('asshur', 8),\n",
       " ('herdmen', 8),\n",
       " ('vale', 8),\n",
       " ('mistress', 8),\n",
       " ('foreskin', 8),\n",
       " ('laughed', 8),\n",
       " ('quarter', 8),\n",
       " ('besides', 8),\n",
       " ('wot', 8),\n",
       " ('sojourner', 8),\n",
       " ('withhold', 8),\n",
       " ('emptied', 8),\n",
       " ('prosperous', 8),\n",
       " ('venison', 8),\n",
       " ('kids', 8),\n",
       " ('blesseth', 8),\n",
       " ('reached', 8),\n",
       " ('descending', 8),\n",
       " ('maiden', 8),\n",
       " ('dinah', 8),\n",
       " ('leap', 8),\n",
       " ('overtook', 8),\n",
       " ('furniture', 8),\n",
       " ('penuel', 8),\n",
       " ('handmaids', 8),\n",
       " ('commune', 8),\n",
       " ('peaceable', 8),\n",
       " ('aholibamah', 8),\n",
       " ('jeush', 8),\n",
       " ('ebal', 8),\n",
       " ('bozrah', 8),\n",
       " ('seventeen', 8),\n",
       " ('feeding', 8),\n",
       " ('onan', 8),\n",
       " ('timnath', 8),\n",
       " ('doer', 8),\n",
       " ('butler', 8),\n",
       " ('baker', 8),\n",
       " ('sprung', 8),\n",
       " ('dearth', 8),\n",
       " ('almonds', 8),\n",
       " ('carmi', 8),\n",
       " ('purchase', 8),\n",
       " ('yielded', 8),\n",
       " ('groaning', 8),\n",
       " ('borrow', 8),\n",
       " ('takest', 8),\n",
       " ('diminish', 8),\n",
       " ('yesterday', 8),\n",
       " ('case', 8),\n",
       " ('regarded', 8),\n",
       " ('dough', 8),\n",
       " ('blast', 8),\n",
       " ('prophetess', 8),\n",
       " ('murmur', 8),\n",
       " ('worms', 8),\n",
       " ('discomfited', 8),\n",
       " ('proudly', 8),\n",
       " ('easier', 8),\n",
       " ('eagles', 8),\n",
       " ('bounds', 8),\n",
       " ('covet', 8),\n",
       " ('duty', 8),\n",
       " ('wont', 8),\n",
       " ('entice', 8),\n",
       " ('unrighteous', 8),\n",
       " ('refreshed', 8),\n",
       " ('backs', 8),\n",
       " ('sapphire', 8),\n",
       " ('fillets', 8),\n",
       " ('hearted', 8),\n",
       " ('broidered', 8),\n",
       " ('ouches', 8),\n",
       " ('span', 8),\n",
       " ('beryl', 8),\n",
       " ('uri', 8),\n",
       " ('defileth', 8),\n",
       " ('stiffnecked', 8),\n",
       " ('powder', 8),\n",
       " ('tied', 8),\n",
       " ('baken', 8),\n",
       " ('lacking', 8),\n",
       " ('reconciliation', 8),\n",
       " ('cheweth', 8),\n",
       " ('deeper', 8),\n",
       " ('appeareth', 8),\n",
       " ('kin', 8),\n",
       " ('molech', 8),\n",
       " ('grape', 8),\n",
       " ('diverse', 8),\n",
       " ('unless', 8),\n",
       " ('shelomith', 8),\n",
       " ('reckon', 8),\n",
       " ('pine', 8),\n",
       " ('ensign', 8),\n",
       " ('nahshon', 8),\n",
       " ('gershonites', 8),\n",
       " ('apiece', 8),\n",
       " ('censers', 8),\n",
       " ('curses', 8),\n",
       " ('dedication', 8),\n",
       " ('waiting', 8),\n",
       " ('hormah', 8),\n",
       " ('purification', 8),\n",
       " ('proverbs', 8),\n",
       " ('chemosh', 8),\n",
       " ('edrei', 8),\n",
       " ('narrow', 8),\n",
       " ('zealous', 8),\n",
       " ('sinful', 8),\n",
       " ('instrument', 8),\n",
       " ('weapon', 8),\n",
       " ('righteously', 8),\n",
       " ('ramoth', 8),\n",
       " ('slack', 8),\n",
       " ('affrighted', 8),\n",
       " ('destroyeth', 8),\n",
       " ('pervert', 8),\n",
       " ('fleeth', 8),\n",
       " ('vesture', 8),\n",
       " ('virginity', 8),\n",
       " ('emerods', 8),\n",
       " ('rooted', 8),\n",
       " ('showers', 8),\n",
       " ('apple', 8),\n",
       " ('stirreth', 8),\n",
       " ('jabin', 8),\n",
       " ('adullam', 8),\n",
       " ('aphek', 8),\n",
       " ('pertaining', 8),\n",
       " ('mareshah', 8),\n",
       " ('outgoings', 8),\n",
       " ('ophrah', 8),\n",
       " ('zorah', 8),\n",
       " ('abdon', 8),\n",
       " ('fearing', 8),\n",
       " ('throw', 8),\n",
       " ('benjamite', 8),\n",
       " ('nail', 8),\n",
       " ('pierced', 8),\n",
       " ('dens', 8),\n",
       " ('outside', 8),\n",
       " ('pursuing', 8),\n",
       " ('trode', 8),\n",
       " ('riddle', 8),\n",
       " ('foxes', 8),\n",
       " ('benjamites', 8),\n",
       " ('sling', 8),\n",
       " ('dance', 8),\n",
       " ('vial', 8),\n",
       " ('carrying', 8),\n",
       " ('michmash', 8),\n",
       " ('honeycomb', 8),\n",
       " ('disperse', 8),\n",
       " ('helmet', 8),\n",
       " ('trench', 8),\n",
       " ('behaved', 8),\n",
       " ('gilboa', 8),\n",
       " ('geshur', 8),\n",
       " ('carpenters', 8),\n",
       " ('gittite', 8),\n",
       " ('thinkest', 8),\n",
       " ('suppose', 8),\n",
       " ('longed', 8),\n",
       " ('bichri', 8),\n",
       " ('giant', 8),\n",
       " ('netophathite', 8),\n",
       " ('araunah', 8),\n",
       " ('uproar', 8),\n",
       " ('harlots', 8),\n",
       " ('wiser', 8),\n",
       " ('diseased', 8),\n",
       " ('talking', 8),\n",
       " ('jezreelite', 8),\n",
       " ('embrace', 8),\n",
       " ('warm', 8),\n",
       " ('box', 8),\n",
       " ('faithfully', 8),\n",
       " ('murderers', 8),\n",
       " ('menahem', 8),\n",
       " ('bridle', 8),\n",
       " ('wipe', 8),\n",
       " ('zabad', 8),\n",
       " ('berechiah', 8),\n",
       " ('shechaniah', 8),\n",
       " ('elioenai', 8),\n",
       " ('akkub', 8),\n",
       " ('plants', 8),\n",
       " ('genealogies', 8),\n",
       " ('jerimoth', 8),\n",
       " ('zebadiah', 8),\n",
       " ('mattithiah', 8),\n",
       " ('psalms', 8),\n",
       " ('kindreds', 8),\n",
       " ('preparation', 8),\n",
       " ('affairs', 8),\n",
       " ('searcheth', 8),\n",
       " ('hissing', 8),\n",
       " ('kadmiel', 8),\n",
       " ('maids', 8),\n",
       " ('magistrates', 8),\n",
       " ('sherebiah', 8),\n",
       " ('persecutors', 8),\n",
       " ('chamberlains', 8),\n",
       " ('bindeth', 8),\n",
       " ('withereth', 8),\n",
       " ('dreadful', 8),\n",
       " ('eyelids', 8),\n",
       " ('vomit', 8),\n",
       " ('island', 8),\n",
       " ('shineth', 8),\n",
       " ('departeth', 8),\n",
       " ('pearls', 8),\n",
       " ('balance', 8),\n",
       " ('flattering', 8),\n",
       " ('pleasures', 8),\n",
       " ('scattereth', 8),\n",
       " ('shaking', 8),\n",
       " ('bent', 8),\n",
       " ('murder', 8),\n",
       " ('bend', 8),\n",
       " ('distresses', 8),\n",
       " ('confident', 8),\n",
       " ('deviseth', 8),\n",
       " ('considereth', 8),\n",
       " ('partaker', 8),\n",
       " ('adulterers', 8),\n",
       " ('judgest', 8),\n",
       " ('create', 8),\n",
       " ('uphold', 8),\n",
       " ('dissolved', 8),\n",
       " ('teachest', 8),\n",
       " ('dross', 8),\n",
       " ('contention', 8),\n",
       " ('hasty', 8),\n",
       " ('meats', 8),\n",
       " ('preacher', 8),\n",
       " ('comforter', 8),\n",
       " ('books', 8),\n",
       " ('pangs', 8),\n",
       " ('rushing', 8),\n",
       " ('languisheth', 8),\n",
       " ('instant', 8),\n",
       " ('fan', 8),\n",
       " ('sail', 8),\n",
       " ('surname', 8),\n",
       " ('surnamed', 8),\n",
       " ('astrologers', 8),\n",
       " ('pastors', 8),\n",
       " ('foreheads', 8),\n",
       " ('grain', 8),\n",
       " ('baptize', 8),\n",
       " ('tormented', 8),\n",
       " ('impossible', 8),\n",
       " ('believing', 8),\n",
       " ('rabbi', 8),\n",
       " ('accusers', 8),\n",
       " ('perdition', 8),\n",
       " ('cyprus', 8),\n",
       " ('boasting', 8),\n",
       " ('earnest', 8),\n",
       " ('perils', 8),\n",
       " ('melchisedec', 8),\n",
       " ('lights', 9),\n",
       " ('dress', 9),\n",
       " ('flaming', 9),\n",
       " ('finding', 9),\n",
       " ('hivite', 9),\n",
       " ('jobab', 9),\n",
       " ('entreated', 9),\n",
       " ('journeys', 9),\n",
       " ('zoar', 9),\n",
       " ('circumcise', 9),\n",
       " ('bake', 9),\n",
       " ('rained', 9),\n",
       " ('saddled', 9),\n",
       " ('provide', 9),\n",
       " ('worth', 9),\n",
       " ('kindly', 9),\n",
       " ('proceedeth', 9),\n",
       " ('purchased', 9),\n",
       " ('point', 9),\n",
       " ('loss', 9),\n",
       " ('booths', 9),\n",
       " ('herein', 9),\n",
       " ('deborah', 9),\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(fre_dist.items(), key = lambda item: item[1]) # 단어의 빈도수를 저장하는 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(fre_dist) # 단어의 수가 저장 되고\n",
    "idx_to_word = {idx: word for idx,  word in enumerate(fre_dist.keys())}\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### convert word to index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus_indexed = [[word_to_idx[word] for word in sent if word in word_to_idx]for sent in corpus]\n",
    "corpus_indexed = [sent for sent in corpus_indexed if len(sent) > 5]\n",
    "fre_dist_indexed = {word_to_idx[w]: f for w, f in fre_dist.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# corpus_indexed[0]\n",
    "# for i in corpus_indexed[0]:\n",
    "#     print(idx_to_word[i])\n",
    "# corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Huffman Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HuffmanNode:\n",
    "    def __init__(self, is_leaf, value=None, fre=0, left=None, right=None):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.value = value  # the node's index in huffman tree / char\n",
    "        self.fre = fre  # word frequency in corpus / freq\n",
    "        self.code = []  # huffman code \n",
    "        self.code_len = 0  # lenght of code\n",
    "        self.node_path = []  # the path from root node to this node\n",
    "        self.left = left  # left child\n",
    "        self.right = right  # right child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "freq_dict = sorted(fre_dist_indexed.items(), key=lambda x:x[1], reverse=True)\n",
    "node_list = [HuffmanNode(is_leaf=True, value=w, fre=fre) for w, fre in freq_dict]  # create leaf node\n",
    "node_list += [HuffmanNode(is_leaf=False, fre=1e10) for i in range(len(fre_dist_indexed))]  # create non-leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class HeapNode:\n",
    "    def __init__(self, char, freq):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.index = None\n",
    "        self.vector = None\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if other is None:\n",
    "            return -1\n",
    "        if not isinstance(other, HeapNode):\n",
    "            return -1\n",
    "        return self.freq < other.freq\n",
    "\n",
    "\n",
    "class HuffmanCoding:\n",
    "    def __init__(self):\n",
    "        self.heap = []\n",
    "        self.codes = {}\n",
    "        self.reverse_mapping = {}\n",
    "        self.merged_nodes = None\n",
    "\n",
    "    def make_heap(self, frequency):  # frequency has a shape of  { word : frequency }\n",
    "        for key in frequency:  # make a node, then push in list of heap queue\n",
    "            node = HeapNode(key, frequency[key])\n",
    "            heapq.heappush(self.heap, node)\n",
    "\n",
    "    def merge_nodes(self):      # make nodes from low to high frequency and merge to tree.\n",
    "        index = 0\n",
    "        merged = None\n",
    "        while len(self.heap) > 1:\n",
    "            node1 = heapq.heappop(self.heap)\n",
    "            node2 = heapq.heappop(self.heap)\n",
    "\n",
    "            merged = HeapNode(None, node1.freq + node2.freq)\n",
    "            merged.left = node1\n",
    "            merged.right = node2\n",
    "            merged.index = index                # index is reversed, i.e. root node has a biggest index.\n",
    "            heapq.heappush(self.heap, merged)\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        return merged\n",
    "\n",
    "    def make_codes_helper(self, root, current_code):\n",
    "        if root is None:\n",
    "            return\n",
    "\n",
    "        if root.char is not None:\n",
    "            self.codes[root.char] = current_code\n",
    "            self.reverse_mapping[current_code] = root.char\n",
    "            return\n",
    "\n",
    "        self.make_codes_helper(root.left, current_code + \"0\")\n",
    "        self.make_codes_helper(root.right, current_code + \"1\")\n",
    "\n",
    "    def make_codes(self):\n",
    "        root = heapq.heappop(self.heap)\n",
    "        current_code = \"\"\n",
    "        self.make_codes_helper(root, current_code)\n",
    "\n",
    "    def build(self, frequency):\n",
    "        self.make_heap(frequency) # frequency를 기준으로 heapnode를 추가한다.\n",
    "        merged = self.merge_nodes()\n",
    "        self.make_codes()\n",
    "\n",
    "        return self.codes, merged\n",
    "\n",
    "\n",
    "def init_huffman_modified():\n",
    "    h = HuffmanCoding()\n",
    "\n",
    "    codes, merged = h.build(fre_dist)\n",
    "    \n",
    "    tree = {}\n",
    "    max_depth = 0\n",
    "\n",
    "    for word in tqdm(codes.keys(), desc='Building Huffman Tree', ncols=100):\n",
    "        direction_code = codes[word]\n",
    "        depth = len(direction_code)\n",
    "        root = merged\n",
    "        index_path = [root.index]\n",
    "        direction_path = []\n",
    "        for i in range(depth):\n",
    "            direction_path.append(int(direction_code[i]))\n",
    "            if direction_code[i] == '0':\n",
    "                root = root.left\n",
    "            else:\n",
    "                root = root.right\n",
    "            if root.index is not None:\n",
    "                index_path.append(root.index)\n",
    "        # if len(index_path) != len(direction_path):\n",
    "        #     print(word)\n",
    "        #     print(direction_code)\n",
    "        #     print(len(direction_code))\n",
    "        #     print(len(index_path))\n",
    "        #     print(len(direction_path))\n",
    "        #     print(index_path)\n",
    "        #     print(direction_path)\n",
    "        #     break\n",
    "        info = {'index_path': index_path, 'direction_path': direction_path, 'depth': depth}\n",
    "        tree[word_to_idx[word]] = info\n",
    "\n",
    "        if depth > max_depth:\n",
    "            max_depth = depth\n",
    "\n",
    "    return tree, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Huffman Tree: 100%|█████████████████████████████████| 4533/4533 [00:00<00:00, 32187.46it/s]\n"
     ]
    }
   ],
   "source": [
    "tree, max_depth = init_huffman_modified()\n",
    "total = sum([item[1] for item in fre_dist.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('./')\n",
    "\n",
    "import argparse\n",
    "from distutils.util import strtobool as _bool\n",
    "import numpy as np\n",
    "\n",
    "from src.preprocess_utils import *\n",
    "from src.huffman import *\n",
    "from eval import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(xs):\n",
    "    ans = 1 / (1 + np.exp(-xs))\n",
    "    top = 1 / (1 + math.exp(6))\n",
    "    bottom = 1 / (1 + math.exp(-6))\n",
    "    for i, num in enumerate(ans[0]):\n",
    "        if num < top:\n",
    "            ans[0, i] = 0\n",
    "        elif num > bottom:\n",
    "            ans[0, i] = 1\n",
    "    return ans\n",
    "\n",
    "\n",
    "def similar_word(emb):\n",
    "    index_to_word = pickle.load(open(cfg.index_to_word_path, 'rb'))\n",
    "    word_to_index = pickle.load(open(cfg.word_to_index_path, 'rb'))\n",
    "    embedding_norm = np.linalg.norm(emb, axis=1)\n",
    "    norm_emb = emb / embedding_norm[:, None]\n",
    "    word1 = word_to_index['king']\n",
    "    word2 = word_to_index['queen']\n",
    "    word3 = word_to_index['husband']\n",
    "    answer = word_to_index['wife']\n",
    "\n",
    "    target = norm_emb[word2] - norm_emb[word1] + norm_emb[word3]\n",
    "    target = target / np.linalg.norm(target)\n",
    "\n",
    "    max_index = answer\n",
    "    max_sim = cosine_similarity(target, norm_emb[answer])\n",
    "    for i in tqdm(range(len(word_to_index)), desc=\"Finding closest word to queen-king+husband\", ncols=70):\n",
    "        if i == word1 or i == word2 or i == word3 or i == answer:\n",
    "            pass\n",
    "        else:\n",
    "            sim = cosine_similarity(norm_emb[i], target)\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                max_index = i\n",
    "    print(index_to_word[max_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "embedding = np.random.uniform(low=-0.5/300, high=0.5/300, size=(vocab_size, hidden_size)).astype('f')\n",
    "emb_grad_temp = []\n",
    "context = np.zeros_like(embedding).astype('f')  # for negative sampling\n",
    "node_mat = np.zeros((vocab_size-1, hidden_size)).astype('f')  # for hierarchical softmax\n",
    "node_mat_grad_temp = []\n",
    "\n",
    "starting_lr = 0.05\n",
    "min_loss = math.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on 4533 words\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training on {} words\".format(vocab_size))\n",
    "step = 0\n",
    "update_step = 0\n",
    "# logging_loss = 0\n",
    "start_time = time.time()\n",
    "lr = starting_lr\n",
    "update_size = 12\n",
    "epochs = 3\n",
    "window_size = 10\n",
    "subsampling_t = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HierarchicalSoftmaxLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, freq_dict):\n",
    "        super().__init__()\n",
    "        ## in w2v c implement, syn1 initial with all zero\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.syn1 = nn.Embedding(\n",
    "            num_embeddings=vocab_size + 1,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=vocab_size\n",
    "\n",
    "        )\n",
    "        torch.nn.init.constant_(self.syn1.weight.data, val=0)\n",
    "        self.huffman_tree = HuffmanTree(freq_dict)\n",
    "\n",
    "    def forward(self, neu1, target):\n",
    "        # neu1: [b_size, embedding_dim]\n",
    "        # target: [b_size, 1]\n",
    "\n",
    "        # turns:[b_size, max_code_len_in_batch]\n",
    "        # paths: [b_size, max_code_len_in_batch]\n",
    "        turns, paths = self._get_turns_and_paths(target)\n",
    "        paths_emb = self.syn1(paths)  # [b_size, max_code_len_in_batch, embedding_dim]\n",
    "\n",
    "        loss = -F.logsigmoid(\n",
    "            (turns.unsqueeze(2) * paths_emb * neu1.unsqueeze(1)).sum(2)).sum(1).mean()\n",
    "        return loss\n",
    "\n",
    "    def _get_turns_and_paths(self, target):\n",
    "        turns = []  # turn right(1) or turn left(-1) in huffman tree\n",
    "        paths = []\n",
    "        max_len = 0\n",
    "        ''' we have batch of center words ... '''\n",
    "        for n in target:\n",
    "            n = n.item()\n",
    "            node = self.huffman_tree.node_dict[n]\n",
    "\n",
    "            code = target.new_tensor(node.code).int()  # in code, left node is 0; right node is 1\n",
    "            turn = torch.where(code == 1, code, -torch.ones_like(code))  # 1 -> 1;  0 -> -1\n",
    "\n",
    "            turns.append(turn)\n",
    "            '''node_path records the index from root to leaf node in huffman tree'''\n",
    "            paths.append(target.new_tensor(node.node_path))\n",
    "\n",
    "            if node.code_len > max_len:\n",
    "                max_len = node.code_len\n",
    "\n",
    "        '''Because each word may has different code length, we should pad them to equal length'''\n",
    "        turns = [F.pad(t, pad=(0, max_len - len(t)), mode='constant', value=0) for t in turns]\n",
    "        paths = [F.pad(p, pad=(0, max_len - p.shape[0]), mode='constant', value=net.hs.vocab_size) for p in paths]\n",
    "        return torch.stack(turns).int(), torch.stack(paths).long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "아래부턴ㅌ 쓰레기\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The way builing huffman tree refer to c's original implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HuffmanTree:\n",
    "    def __init__(self, freq_dict):\n",
    "        self.root = None\n",
    "        freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True) # 내림차순으로 나열\n",
    "        self.vocab_size = len(freq_dict)\n",
    "        self.node_dict = {}\n",
    "        self._build_tree(freq_dict)\n",
    "    \n",
    "    def _build_tree(self, freq_dict):\n",
    "        '''\n",
    "            freq_dict is in 내림차순 정렬\n",
    "            node_list: two part: [leaf node :: internal node]\n",
    "                leaf node is sorting by frequency in decent order; \n",
    "        '''\n",
    "    \n",
    "        node_list = [HuffmanNode(is_leaf=True, value=w, fre=fre) for w, fre in freq_dict]  # create leaf node\n",
    "        node_list += [HuffmanNode(is_leaf=False, fre=1e10) for i in range(self.vocab_size)]  # create non-leaf node\n",
    "\n",
    "        parentNode = [0] * (self.vocab_size * 2)  # only 2 * vocab_size - 2 be used\n",
    "        binary = [0] * (self.vocab_size * 2)  # recording turning left or turning right\n",
    "        \n",
    "        '''\n",
    "          pos1 points to currently processing leaf node at left side of node_list\n",
    "          pos2 points to currently processing non-leaf node at right side of node_list\n",
    "        '''\n",
    "\n",
    "        pos1 = self.vocab_size - 1\n",
    "        pos2 = self.vocab_size\n",
    "        \n",
    "        '''\n",
    "            each iteration picks two node from node_list\n",
    "            the first pick assigns to min1i\n",
    "            the second pick assigns to min2i \n",
    "            \n",
    "            min2i's frequency is always larger than min1i\n",
    "        '''\n",
    "        min1i = 0\n",
    "        min2i = 0\n",
    "        '''\n",
    "            the main process of building huffman tree\n",
    "        '''\n",
    "        for a in range(self.vocab_size - 1):\n",
    "            '''\n",
    "                first pick assigns to min1i\n",
    "            '''\n",
    "            if pos1 >= 0:\n",
    "                if node_list[pos1].fre < node_list[pos2].fre:\n",
    "                    min1i = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min1i = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min1i = pos2\n",
    "                pos2 += 1\n",
    "            \n",
    "            '''\n",
    "               second pick assigns to min2i \n",
    "            '''\n",
    "            if pos1 >= 0:\n",
    "                if node_list[pos1].fre < node_list[pos2].fre:\n",
    "                    min2i = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min2i = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min2i = pos2\n",
    "                pos2 += 1\n",
    "            \n",
    "            ''' fill information of non leaf node '''\n",
    "            node_list[self.vocab_size + a].fre = node_list[min1i].fre + node_list[min2i].fre\n",
    "            node_list[self.vocab_size + a].left = node_list[min1i]\n",
    "            node_list[self.vocab_size + a].right = node_list[min2i]\n",
    "            \n",
    "            '''\n",
    "                the parent node always is non leaf node\n",
    "                assigen lead child (min2i) and right child (min1i) to parent node\n",
    "            '''\n",
    "            parentNode[min1i] = self.vocab_size + a  # max index = 2 * vocab_size - 2\n",
    "            parentNode[min2i] = self.vocab_size + a\n",
    "            binary[min2i] = 1\n",
    "        \n",
    "        '''generate huffman code of each leaf node '''\n",
    "        for a in range(self.vocab_size):\n",
    "            b = a\n",
    "            i = 0\n",
    "            code = []\n",
    "            point = []\n",
    "\n",
    "            '''\n",
    "\n",
    "                backtrace path from current node until root node. (bottom up)\n",
    "                'root node index' in node_list is  2 * vocab_size - 2 \n",
    "            '''\n",
    "            while b != self.vocab_size * 2 - 2:\n",
    "                code.append(binary[b])  \n",
    "                b = parentNode[b]\n",
    "                # point recording the path index from leaf node to root, the length of point is less 1 than the length of code\n",
    "                point.append(b)\n",
    "            \n",
    "            '''\n",
    "                huffman code should be top down, so we reverse it.\n",
    "            '''\n",
    "            node_list[a].code_len = len(code)\n",
    "            node_list[a].code = list(reversed(code))\n",
    "            \n",
    "\n",
    "            '''\n",
    "                1. Recording the path from root to leaf node (top down). \n",
    "                \n",
    "                2.The actual index value should be shifted by self.vocab_size,\n",
    "                  because we need the index starting from zero to mapping non-leaf node\n",
    "                \n",
    "                3. In case of full binary tree, the number of non leaf node always equals to vocab_size - 1.\n",
    "                  The index of BST root node in node_list is 2 * vocab_size - 2,\n",
    "                  and we shift vocab_size to get the actual index of root node: vocab_size - 2\n",
    "            '''\n",
    "            node_list[a].node_path = list(reversed([p - self.vocab_size for p in point]))\n",
    "            \n",
    "            self.node_dict[node_list[a].value] = node_list[a]\n",
    "            \n",
    "        self.root = node_list[2 * vocab_size - 2]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CBOW + HS\n",
    "$$\n",
    "-\\log p(w_O| w_I) = -\\log \\dfrac{\\text{exp}({h^\\top \\text{v}'_O})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})}= - \\sum^{L(w)-1}_{l=1}  \\log\\sigma( [ \\cdot ] h^\\top \\text{v}^{'}_l)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### create dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CBOWDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, windows_size=5, sentence_length_threshold=5):\n",
    "        self.windows_size = windows_size\n",
    "        self.sentence_length_threshold = sentence_length_threshold\n",
    "        self.contexts, self.centers = self._generate_pairs(corpus, windows_size)\n",
    "        \n",
    "    def _generate_pairs(self, corpus, windows_size):\n",
    "        contexts = []\n",
    "        centers = []\n",
    "        \n",
    "        for sent in corpus:\n",
    "            if len(sent) < self.sentence_length_threshold:\n",
    "                continue\n",
    "            \n",
    "            for center_word_pos in range(len(sent)):\n",
    "                context = []\n",
    "                for w in range(-windows_size, windows_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    if(0 <= context_word_pos < len(sent) and context_word_pos != center_word_pos):\n",
    "                        context.append(sent[context_word_pos])\n",
    "                if(len(context) == 2 * self.windows_size):\n",
    "                    contexts.append(context)\n",
    "                    centers.append(sent[center_word_pos])\n",
    "        return contexts, centers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return np.array(self.contexts[index]), np.array([self.centers[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HierarchicalSoftmaxLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, freq_dict):\n",
    "        super().__init__()\n",
    "        ## in w2v c implement, syn1 initial with all zero\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.syn1 = nn.Embedding(\n",
    "            num_embeddings=vocab_size + 1,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=vocab_size\n",
    "            \n",
    "        )\n",
    "        torch.nn.init.constant_(self.syn1.weight.data, val=0)\n",
    "        self.huffman_tree = HuffmanTree(freq_dict)\n",
    "\n",
    "    def forward(self, neu1, target):\n",
    "        # neu1: [b_size, embedding_dim]\n",
    "        # target: [b_size, 1]\n",
    "        \n",
    "        # turns:[b_size, max_code_len_in_batch]\n",
    "        # paths: [b_size, max_code_len_in_batch]\n",
    "        turns, paths = self._get_turns_and_paths(target)\n",
    "        paths_emb = self.syn1(paths) # [b_size, max_code_len_in_batch, embedding_dim]\n",
    "\n",
    "        loss = -F.logsigmoid(\n",
    "            (turns.unsqueeze(2) * paths_emb * neu1.unsqueeze(1)).sum(2)).sum(1).mean()\n",
    "        return loss\n",
    "    \n",
    "    def _get_turns_and_paths(self, target):\n",
    "        turns = []  # turn right(1) or turn left(-1) in huffman tree\n",
    "        paths = []\n",
    "        max_len = 0\n",
    "        ''' we have batch of center words ... '''\n",
    "        for n in target:\n",
    "            n = n.item()\n",
    "            node = self.huffman_tree.node_dict[n]\n",
    "            \n",
    "            code = target.new_tensor(node.code).int()  # in code, left node is 0; right node is 1\n",
    "            turn = torch.where(code == 1, code, -torch.ones_like(code)) # 1 -> 1;  0 -> -1\n",
    "            \n",
    "            turns.append(turn)\n",
    "            '''node_path records the index from root to leaf node in huffman tree'''\n",
    "            paths.append(target.new_tensor(node.node_path))\n",
    "            \n",
    "            if node.code_len > max_len:\n",
    "                max_len = node.code_len\n",
    "        \n",
    "        '''Because each word may has different code length, we should pad them to equal length'''\n",
    "        turns = [F.pad(t, pad=(0, max_len - len(t)), mode='constant', value=0) for t in turns] \n",
    "        paths = [F.pad(p, pad=(0, max_len - p.shape[0]), mode='constant', value=net.hs.vocab_size) for p in paths]\n",
    "        return torch.stack(turns).int(), torch.stack(paths).long()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CBOWHierarchicalSoftmax(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, freq_dict):\n",
    "        super().__init__()\n",
    "        self.syn0 = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hs = HierarchicalSoftmaxLayer(vocab_size, embedding_dim, freq_dict)\n",
    "\n",
    "    \n",
    "    def forward(self, context, target):\n",
    "        # context: [b_size, 2 * window_size]\n",
    "        # target: [b_size]\n",
    "        neu1 = self.syn0(context.long()).mean(dim=1)  # [b_size, embedding_dim]\n",
    "        loss = self.hs(neu1, target.long())\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_set = CBOWDataset(corpus_indexed)\n",
    "data_loader = DataLoader(data_set, batch_size=100, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "net = CBOWHierarchicalSoftmax(vocab_size, embedding_dim, fre_dist_indexed)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001,  weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1147/1147 [00:32<00:00, 35.12it/s, loss=10.9]\n",
      "100%|██████████| 1147/1147 [00:29<00:00, 38.94it/s, loss=10.7]\n",
      "100%|██████████| 1147/1147 [00:30<00:00, 37.59it/s, loss=10.6]\n",
      "100%|██████████| 1147/1147 [00:22<00:00, 50.96it/s, loss=10.5]\n",
      "100%|██████████| 1147/1147 [00:29<00:00, 39.32it/s, loss=10.4]\n",
      "100%|██████████| 1147/1147 [00:26<00:00, 43.69it/s, loss=10.3]\n",
      "100%|██████████| 1147/1147 [00:23<00:00, 47.86it/s, loss=10.2]\n",
      "100%|██████████| 1147/1147 [00:26<00:00, 43.45it/s, loss=10.1]\n",
      "100%|██████████| 1147/1147 [00:27<00:00, 41.68it/s, loss=10.1]\n",
      "100%|██████████| 1147/1147 [00:28<00:00, 40.07it/s, loss=10]\n"
     ]
    }
   ],
   "source": [
    "log_interval = 100\n",
    "for epoch_i in range(10):\n",
    "    total_loss = 0\n",
    "    net.train()\n",
    "    tk0 = tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (context, center) in enumerate(tk0):\n",
    "\n",
    "        loss = net(context, center)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if(i + 1) % log_interval == 0:\n",
    "            tk0.set_postfix(loss = total_loss/log_interval)\n",
    "            total_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## fetch word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiangchiuti/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "w2v_embedding = net.syn0.weight.data\n",
    "w2v_embedding = w2v_embedding.numpy()\n",
    "l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)\n",
    "w2v_embedding = w2v_embedding / l2norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CosineSimilarity:\n",
    "    def __init__(self, word_embedding, idx_to_word_dict, word_to_idx_dict):\n",
    "        self.word_embedding = word_embedding # normed already\n",
    "        self.idx_to_word_dict = idx_to_word_dict\n",
    "        self.word_to_idx_dict = word_to_idx_dict\n",
    "        \n",
    "    def get_synonym(self, word, topK=10):\n",
    "        idx = self.word_to_idx_dict[word]\n",
    "        embed = self.word_embedding[idx]\n",
    "        \n",
    "        cos_similairty = w2v_embedding @ embed\n",
    "        \n",
    "        topK_index = np.argsort(-cos_similairty)[:topK]\n",
    "        pairs = []\n",
    "        for i in topK_index:\n",
    "            w = self.idx_to_word_dict[i]\n",
    "            pairs.append((w, cos_similairty[i]))\n",
    "        return pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('christ', 1.0),\n",
       " ('hope', 0.78780156),\n",
       " ('gospel', 0.7656436),\n",
       " ('jesus', 0.74575657),\n",
       " ('faith', 0.7190881),\n",
       " ('godliness', 0.7005944),\n",
       " ('offences', 0.70045626),\n",
       " ('grace', 0.6946964),\n",
       " ('dear', 0.666232),\n",
       " ('willing', 0.66131693)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)\n",
    "cosinSim.get_synonym('christ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('god', 1.0),\n",
       " ('saviour', 0.53627664),\n",
       " ('remember', 0.51367503),\n",
       " ('sure', 0.4997003),\n",
       " ('hope', 0.47002873),\n",
       " ('purpose', 0.46906227),\n",
       " ('praise', 0.45354468),\n",
       " ('thanks', 0.4486973),\n",
       " ('doubtless', 0.44689322),\n",
       " ('formed', 0.44300675)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinSim.get_synonym('god')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jesus', 0.9999999),\n",
       " ('gospel', 0.8051339),\n",
       " ('grace', 0.75879383),\n",
       " ('church', 0.7542972),\n",
       " ('christ', 0.74575657),\n",
       " ('manifest', 0.7415799),\n",
       " ('believed', 0.7215627),\n",
       " ('faith', 0.7198993),\n",
       " ('godliness', 0.7091305),\n",
       " ('john', 0.7015951)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinSim.get_synonym('jesus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf25')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "38d8e74da4dd4c9c60ac9cb623ee728ede57d0760220b21541448ba462b2b94b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}